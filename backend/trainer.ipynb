{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1319</td>\n",
       "      <td>5/19/2022 18:16</td>\n",
       "      <td>transaction_imps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>830</td>\n",
       "      <td>1/23/2022 20:11</td>\n",
       "      <td>priority_banking_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483</td>\n",
       "      <td>12/3/2020 9:07</td>\n",
       "      <td>FOR_add_beneficiary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1496</td>\n",
       "      <td>12/27/2022 20:25</td>\n",
       "      <td>transaction_imps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1213</td>\n",
       "      <td>9/10/2021 22:05</td>\n",
       "      <td>gold_loan_renew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id         timestamp                    action\n",
       "0         1319   5/19/2022 18:16          transaction_imps\n",
       "1          830   1/23/2022 20:11  priority_banking_request\n",
       "2         1483    12/3/2020 9:07       FOR_add_beneficiary\n",
       "3         1496  12/27/2022 20:25          transaction_imps\n",
       "4         1213   9/10/2021 22:05           gold_loan_renew"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/embedded_dataset.csv')\n",
    "pd.set_option('display.max_rows', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21805/21805 [==============================] - 139s 6ms/step - loss: 3.5272 - accuracy: 0.0290 - val_loss: 3.5268 - val_accuracy: 0.0291\n",
      "Epoch 2/50\n",
      "21805/21805 [==============================] - 202s 9ms/step - loss: 3.5267 - accuracy: 0.0295 - val_loss: 3.5267 - val_accuracy: 0.0291\n",
      "Epoch 3/50\n",
      "21805/21805 [==============================] - 213s 10ms/step - loss: 3.5265 - accuracy: 0.0305 - val_loss: 3.5270 - val_accuracy: 0.0290\n",
      "Epoch 4/50\n",
      "21805/21805 [==============================] - 135s 6ms/step - loss: 3.5262 - accuracy: 0.0309 - val_loss: 3.5272 - val_accuracy: 0.0292\n",
      "Epoch 5/50\n",
      "21799/21805 [============================>.] - ETA: 0s - loss: 3.5258 - accuracy: 0.0316"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Unable to create file (unable to open file: name = '/checkpoint_05.h5', errno = 13, error message = 'Permission denied', flags = 13, o_flags = 302)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TANU SHREE\\OneDrive\\Desktop\\SIB'23\\sib\\sib\\trainer.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TANU%20SHREE/OneDrive/Desktop/SIB%2723/sib/sib/trainer.ipynb#W1sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TANU%20SHREE/OneDrive/Desktop/SIB%2723/sib/sib/trainer.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TANU%20SHREE/OneDrive/Desktop/SIB%2723/sib/sib/trainer.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), callbacks\u001b[39m=\u001b[39;49m[model_checkpoint])\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\h5py\\_hl\\files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    558\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    560\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[0;32m    561\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[0;32m    562\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[0;32m    563\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    564\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[0;32m    565\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[0;32m    566\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 567\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[0;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\h5py\\_hl\\files.py:237\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    235\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mcreate(name, h5f\u001b[39m.\u001b[39mACC_EXCL, fapl\u001b[39m=\u001b[39mfapl, fcpl\u001b[39m=\u001b[39mfcpl)\n\u001b[0;32m    236\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 237\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mcreate(name, h5f\u001b[39m.\u001b[39;49mACC_TRUNC, fapl\u001b[39m=\u001b[39;49mfapl, fcpl\u001b[39m=\u001b[39;49mfcpl)\n\u001b[0;32m    238\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    239\u001b[0m     \u001b[39m# Open in append mode (read/write).\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[39m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[39m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:126\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Unable to create file (unable to open file: name = '/checkpoint_05.h5', errno = 13, error message = 'Permission denied', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "# Sequential LSTM with checkpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the path where you want to save the model checkpoints\n",
    "checkpoint_path = '/checkpoint_{epoch:02d}.h5'\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,          # Path to save the model\n",
    "    save_best_only=False,              # Set to True if you only want to save the best model\n",
    "    save_weights_only=True,            # Set to True if you only want to save model weights\n",
    "    period=5                           # Save every 5 epochs (adjust this interval as needed)\n",
    ")\n",
    "\n",
    "# One-hot encode the 'action' labels\n",
    "df_encoded = pd.get_dummies(df, columns=['action'], prefix='action')\n",
    "\n",
    "# Create sequences for each customer\n",
    "sequence_length = 5  # Define the sequence length\n",
    "sequences = []\n",
    "\n",
    "for customer_id, group in df_encoded.groupby('customer_id'):\n",
    "    actions = group.drop(['customer_id', 'timestamp'], axis=1).values\n",
    "    for i in range(len(actions) - sequence_length + 1):\n",
    "        sequence = actions[i:i+sequence_length]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "# Split sequences into input and target\n",
    "X = [sequence[:-1] for sequence in sequences]\n",
    "y = [sequence[-1] for sequence in sequences]\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Add a Dense output layer\n",
    "model.add(Dense(34, activation='softmax'))  # Use 34 for multiple action prediction\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume checkpoint training\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model = load_model('checkpoint_50.h5')\n",
    "\n",
    "# Continue training\n",
    "model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test Accuracy: 70.91%\n",
      "Epoch 2, Test Accuracy: 71.07%\n",
      "Epoch 3, Test Accuracy: 70.95%\n",
      "Epoch 4, Test Accuracy: 71.15%\n",
      "Epoch 5, Test Accuracy: 70.98%\n",
      "Epoch 6, Test Accuracy: 71.12%\n",
      "Epoch 7, Test Accuracy: 71.12%\n",
      "Epoch 8, Test Accuracy: 70.99%\n",
      "Epoch 9, Test Accuracy: 71.15%\n",
      "Epoch 10, Test Accuracy: 71.16%\n"
     ]
    }
   ],
   "source": [
    "# LSTM Sequential with Pytorch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# One-hot encode the 'action' labels\n",
    "df_encoded = pd.get_dummies(df, columns=['action'], prefix='action')\n",
    "\n",
    "# Sort the dataset by 'customer_id' and 'timestamp'\n",
    "df_encoded = df_encoded.sort_values(by=['customer_id', 'timestamp'])\n",
    "\n",
    "# Create sequences for each customer\n",
    "sequence_length = 5  # Define the sequence length\n",
    "sequences = []\n",
    "\n",
    "for customer_id, group in df_encoded.groupby('customer_id'):\n",
    "    actions = group.drop(['customer_id', 'timestamp'], axis=1).values\n",
    "    for i in range(len(actions) - sequence_length + 1):\n",
    "        sequence = actions[i:i+sequence_length]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "# Split sequences into input and target\n",
    "X = [sequence[:-1] for sequence in sequences]\n",
    "y = [sequence[-1] for sequence in sequences]\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32  # You can adjust this as needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define a simple LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model and move it to the GPU\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 128\n",
    "num_classes = y_train.shape[1]  # Adjust based on the number of classes in 'action'\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch + 1}, Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '2M_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
